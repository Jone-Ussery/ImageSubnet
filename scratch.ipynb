{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import torch\n",
    "import typing\n",
    "import pydantic\n",
    "import bittensor as bt\n",
    "\n",
    "# Stable diffusion\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
    "from typing import List, Dict, Union, Tuple, Optional\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "bt.debug()\n",
    "\n",
    "# Lets instantiate the stable diffusion model.\n",
    "model =  StableDiffusionPipeline.from_pretrained( \"Lykon/DreamShaper\", custom_pipeline=\"lpw_stable_diffusion\", torch_dtype=torch.float16 ).to('cuda')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "from protocol import TextToImage\n",
    "\n",
    "def text_to_image( synapse: TextToImage ) -> TextToImage:\n",
    "\n",
    "    seed = synapse.seed\n",
    "\n",
    "    if(seed == -1):\n",
    "        seed = torch.randint(1000000000, (1,)).item()\n",
    "\n",
    "    generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "    output = model(\n",
    "        prompt = synapse.text,\n",
    "        height = synapse.height,\n",
    "        width = synapse.width,\n",
    "        num_images_per_prompt = synapse.num_images_per_prompt,\n",
    "        num_inference_steps = synapse.num_inference_steps,\n",
    "        guidance_scale = synapse.guidance_scale,\n",
    "        negative_prompt = synapse.negative_prompt,\n",
    "        generator = generator\n",
    "    )\n",
    "    \n",
    "    image = output.images[0]\n",
    "  \n",
    "    # transform = transforms.PILToTensor()\n",
    "    # Convert the PIL image to Torch tensor\n",
    "    img_tensor = transform(image)\n",
    "    print(img_tensor.shape)\n",
    "    synapse.images = [bt.Tensor.serialize( img_tensor )]\n",
    "    \n",
    "    return synapse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon = bt.axon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon.attach( text_to_image ).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = bt.dendrite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = zip([1,2,3,4], [\"a\", \"b\", \"c\", \"d\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = await d(axon, TextToImage( text = \"A beautiful landscape with a mountain in the background.\", negative_prompt=\"\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = resp.images[0].deserialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToPILImage()(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "# load a fine-tuned image captioning model and corresponding tokenizer and image processor\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# let's perform inference on an image\n",
    "\n",
    "image = img\n",
    "\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# autoregressively generate caption (uses greedy decoding by default)\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, RobertaModel\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"sunny hill\", return_tensors=\"pt\")\n",
    "inputs_alt = tokenizer(\"hill with clouds\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "outputs_alt = model(**inputs_alt)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state[-1][-1]\n",
    "output_embedding = last_hidden_states\n",
    "output_embedding_alt = outputs_alt.last_hidden_state[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embedding = last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(resp.text, return_tensors=\"pt\")\n",
    "print(resp.text)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "input_embedding = last_hidden_states[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cosine_similarity(input_embedding.reshape((1, -1)).detach().numpy(), output_embedding.reshape((1, -1)).detach().numpy()).item(), cosine_similarity(input_embedding.reshape((1, -1)).detach().numpy(), output_embedding_alt.reshape((1, -1)).detach().numpy()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable diffusion\n",
    "import torch\n",
    "import transformers\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
    "\n",
    "# Lets instantiate the stable diffusion model.\n",
    "model =  StableDiffusionPipeline.from_pretrained( \"Lykon/DreamShaper\", custom_pipeline=\"lpw_stable_diffusion\", torch_dtype=torch.float16, safety_checker=None ).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(prompt=\"A sunny day\", num_images_per_prompt=4).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ImageReward as RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric.utils import get_free_gpu, tile_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = RM.load(\"ImageReward-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ranking, rewards = reward_model.inference_rank(\"porn\", outputs)\n",
    "    print(ranking, rewards)\n",
    "    # convert list to tensor\n",
    "    rewards = torch.tensor(rewards)\n",
    "    # normalize rewards\n",
    "    rewards = (rewards - rewards.min()) / (rewards.max() - rewards.min())\n",
    "    rewards = rewards / torch.sum(rewards)\n",
    "    print(ranking, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_images(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = output.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "  \n",
    "# transform = transforms.PILToTensor()\n",
    "# Convert the PIL image to Torch tensor\n",
    "img_tensor = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
